{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet_65.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kKCZKCvxwcdH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Opyd_G9mwXs4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from zipfile import ZipFile\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vz255ihwoCh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bVr6JddnBsG7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "listed = drive.ListFile().GetList()\n",
        "for file in listed:\n",
        "    print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LW0AEY1twrbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "downloaded = drive.CreateFile({'id': '11j1LyGLuq-HIHEgPNlzcxg0pO57rBA2y'})\n",
        "downloaded.GetContentFile('train.zip')\n",
        "with ZipFile(\"train.zip\", 'r') as z:\n",
        "    z.extractall()\n",
        "os.remove(\"train.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T05EtlovwXtF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TGSSaltTrainDataset(Dataset):\n",
        "    def __init__(self, image_dir,mask_dir,depth_csv,train_csv):\n",
        "        self.image_dir=image_dir\n",
        "        self.mask_dir=mask_dir\n",
        "        \n",
        "#         depth=pd.read_csv(depth_csv)\n",
        "#         depth[\"z\"]=(depth[\"z\"]-depth[\"z\"].min())/(depth[\"z\"].max()-depth[\"z\"].min())\n",
        "        self.filter = np.array([(0,-1,-1,-1),(1,0,0,0),(1,0,1,0),(0,1,0,1)])/8\n",
        "        self.input = pd.read_csv(train_csv)\n",
        "        self.input['z'] = (self.input['z']-self.input['z'].min())/(self.input['z'].max()-self.input['z'].min())\n",
        "        self.input.drop(['rle_mask'],axis=1,inplace=True)\n",
        "#         self.input = self.input.merge(depth,how=\"left\",on=\"id\")\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir,self.input.iloc[idx,0]+\".png\")\n",
        "        img = cv2.imread(img_name)\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        img = torch.tensor(img).view(101,101).float()\n",
        "      \n",
        "        mask_name = os.path.join(self.mask_dir,self.input.iloc[idx,0]+\".png\")\n",
        "        mask = cv2.imread(mask_name)\n",
        "        mask = cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\n",
        "        mask = torch.tensor(mask).float()/255\n",
        "        depth = self.input.iloc[idx,1].reshape(1)\n",
        "        return img,mask,depth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bgQNyCiXwXtM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = TGSSaltTrainDataset(\"train/images\",\"train/masks\",\"depths.csv\",\"train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZJgJYAKsoiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True,up_size=None):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            if up_size:\n",
        "                self.up = nn.Upsample(size= up_size, mode='bilinear', align_corners=True)\n",
        "            else:\n",
        "                self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(x2, (diffX // 2 , int(diffX / 2),\n",
        "                        diffY // 2, int(diffY / 2)))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "class conv3d(nn.Module):\n",
        "    '''\n",
        "    Takes a image with given number of channels and performs 3d convolution and then returns the image as 2d\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(conv3d, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 4, 2,stride=2,padding=1)\n",
        "        self.conv2 = nn.Conv3d(4, 16 ,4,stride=2,padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x.unsqueeze_(1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(-1,512,6,6)\n",
        "        return x\n",
        "\n",
        "class DN(nn.Module):\n",
        "    '''\n",
        "    @IN\n",
        "    img: A tensor representing the grayscaled image; shape:[batch_size,1,101,101]\n",
        "    d: The z index of the image\n",
        "    \n",
        "    @OUT\n",
        "    shape:[batch_size,1,101,101] tensor representing the features found by kernel(3*3) created from depth value\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(DN,self).__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Linear(4,9,bias=False),\n",
        "                                   nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(nn.Linear(9,9,bias=True),\n",
        "                                   nn.Sigmoid())\n",
        "        self.conv1 = nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "        self.non_linear1 = nn.ReLU()\n",
        "    def forward(self,x,d):\n",
        "        d= d.view(1)\n",
        "        ker = torch.tensor([d,d**2,d**3,1]).float().cuda()\n",
        "#         print(ker)\n",
        "        ker = self.layer1(ker)\n",
        "        ker = self.layer2(ker).view(1,1,3,3)\n",
        "        self.conv1.weight = nn.Parameter(ker)\n",
        "        x = self.non_linear1(self.conv1(x))\n",
        "        return x\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self,debug=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.debug = debug\n",
        "        self.inc = inconv(1, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256,up_size = (12,12))\n",
        "        self.up2 = up(512, 128,up_size = (25,25))\n",
        "        self.up3 = up(256, 64,up_size = (50,50))\n",
        "        self.up4 = up(128, 64,up_size = (101,101))\n",
        "        self.outc = outconv(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x.unsqueeze_(1)\n",
        "        if self.debug: print(x.shape)\n",
        "        x1 = self.inc(x)\n",
        "        if self.debug: print(x1.shape)\n",
        "        x2 = self.down1(x1)\n",
        "        if self.debug: print(x2.shape)\n",
        "        x3 = self.down2(x2)\n",
        "        if self.debug: print(x3.shape)\n",
        "        x4 = self.down3(x3)\n",
        "        if self.debug: print(x4.shape)\n",
        "        x5 = self.down4(x4)\n",
        "        if self.debug: print(x5.shape)\n",
        "        if self.debug: print('before up')\n",
        "        x = self.up1(x5, x4)\n",
        "        if self.debug: print(x.shape)\n",
        "        x = self.up2(x, x3)\n",
        "        if self.debug: print(x.shape)\n",
        "        x = self.up3(x, x2)\n",
        "        if self.debug: print(x.shape)\n",
        "        x = self.up4(x, x1)\n",
        "        if self.debug: print(x.shape)\n",
        "        x = self.outc(x)\n",
        "        if self.debug: print(x.shape)\n",
        "        return x.squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aa1Q2zJBsyan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class UDNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UDNet,self).__init__()\n",
        "#         self.dn = DN()\n",
        "#         self.un1 = UNet(debug=True)\n",
        "#         self.un2 = UNet(debug=True)\n",
        "        self.un1 = UNet(debug=False)\n",
        "#         self.un2 = UNet(debug=False)\n",
        "    def forward(self,x,d):\n",
        "        x = self.un1(x)\n",
        "#         x.unsqueeze_(1)\n",
        "#         x = self.dn(x,d)\n",
        "#         x.squeeze_(1)\n",
        "#         x = self.un2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G68GNir2tAbI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bef2764-8c14-4adb-8a7a-d25c047dd0f2"
      },
      "cell_type": "code",
      "source": [
        "model = UDNet().cuda()\n",
        "x = torch.randn(10,101,101).cuda()\n",
        "print(model(x,torch.tensor(train_dataset[1][2][0])).shape)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 101, 101])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HPrgWtu2wXtU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "validation_split = 0.2\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(train_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "validation_sampler = SubsetRandomSampler(val_indices)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,sampler=validation_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILpmwaDtwXth",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BinaryCrossEntropyLoss2d(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        \"\"\"\n",
        "        Binary cross entropy loss 2D\n",
        "        Args:\n",
        "            weight:\n",
        "            size_average:\n",
        "        \"\"\"\n",
        "        super(BinaryCrossEntropyLoss2d, self).__init__()\n",
        "        self.bce_loss = nn.BCELoss(weight, size_average)\n",
        "        if torch.cuda.is_available():\n",
        "            self.bce_loss = self.bce_loss.cuda()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = F.sigmoid(pred)\n",
        "        pred = pred.view(-1)  # Flatten\n",
        "        target = target.view(-1)  # Flatten\n",
        "        return self.bce_loss(pred, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLvCr3IQwXtm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SoftDiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftDiceLoss, self).__init__()\n",
        "    def forward(self, pred, target):\n",
        "        smooth = 1\n",
        "        num = target.size(0)\n",
        "        pred = F.sigmoid(pred)\n",
        "        pred = pred.view(num, -1)\n",
        "        target = target.view(num, -1)\n",
        "        intersection = (pred * target)\n",
        "        score = 2. * (intersection.sum(1) + smooth) / (pred.sum(1) + target.sum(1) + smooth)\n",
        "        score = 1 - score.sum() / num\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9CU7PcdGwXtt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dice_coeff(pred, target):\n",
        "    smooth = 1.\n",
        "    num = target.size(0)\n",
        "    pred = pred.view(num, -1)  # Flatten\n",
        "    target = target.view(num, -1)  # Flatten\n",
        "    intersection = (pred * target)\n",
        "    score = (2. * intersection.sum(1) + smooth).float() / (pred.sum(1) + target.sum(1) + smooth).float()\n",
        "    return score.sum()/num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcxIxXq5wXtz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return 0.5*BinaryCrossEntropyLoss2d()(y_true, y_pred)-dice_coeff(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OL9OQNTawXt-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model=UDNet()\n",
        "criterion = SoftDiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krdeJDd5wXuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(threshold):\n",
        "    total_loss = 0\n",
        "    accuracy = 0\n",
        "    model.eval()\n",
        "    for batch_idx, (data,target,d) in enumerate(validation_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "        # forward\n",
        "        output = model(data,d)\n",
        "        predict = (F.sigmoid(output) > threshold).float()\n",
        "        # backward + optimize\n",
        "        loss = criterion(predict, target)\n",
        "        # print statistics\n",
        "        accuracy += dice_coeff(predict, target).item()\n",
        "        total_loss+=loss.item()\n",
        "    print('Validation Loss: {:.5f} Validation Accuracy: {:.5f}'.format(total_loss*batch_size/len(val_indices),accuracy*batch_size/len(val_indices)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ci5neK7wXuU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(threshold):\n",
        "    epoch=1\n",
        "    while True:\n",
        "        total_loss = 0\n",
        "        total_accuracy = 0\n",
        "        model.train()\n",
        "        exp_lr_scheduler.step()\n",
        "        print(exp_lr_scheduler.get_lr())\n",
        "        for batch_idx, (data,target,d) in enumerate(train_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                data = data.cuda()\n",
        "                target = target.cuda()\n",
        "            # forward\n",
        "            output = model(data,d)\n",
        "            predict = (F.sigmoid(output) > threshold).float()\n",
        "            # backward + optimize\n",
        "            loss = criterion(output, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print statistics\n",
        "            accuracy = dice_coeff(predict, target)\n",
        "            total_accuracy+=accuracy\n",
        "            total_loss+=loss\n",
        "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.5f}\\tAccuracy: {:.5f}'.format(epoch, (batch_idx + 1) * len(data), len(train_indices),100*(batch_idx + 1)* len(data) / len(train_indices), total_loss.item(),accuracy))\n",
        "#             if batch_idx%5 == 0:\n",
        "#                 print(str(batch_idx/len(train_indices)*100)+'% completed')\n",
        "    \n",
        "#              print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.5f}\\tAccuracy: {:.5f}'.format(epoch, (batch_idx + 1) * len(data), len(train_indices),100*(batch_idx + 1)* len(data) / len(train_indices), total_loss.item(),accuracy))\n",
        "        print('Train Loss: {:.5f} Train Accuracy: {:.5f}'.format(total_loss.item()*batch_size/len(train_indices),total_accuracy.item()*batch_size/len(train_indices)))\n",
        "        validate(threshold)\n",
        "        epoch+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1XQF7jq5wXud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2056
        },
        "outputId": "500cd92e-1b30-4205-9280-228e3dfd6169"
      },
      "cell_type": "code",
      "source": [
        "train(0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01]\n",
            "Epoch: 1 [100/12800 (1%)]\tLoss: 0.80632\tAccuracy: 0.13122\n",
            "Epoch: 1 [200/12800 (2%)]\tLoss: 1.58609\tAccuracy: 0.22974\n",
            "Epoch: 1 [300/12800 (2%)]\tLoss: 2.34998\tAccuracy: 0.29097\n",
            "Epoch: 1 [400/12800 (3%)]\tLoss: 3.07022\tAccuracy: 0.33257\n",
            "Epoch: 1 [500/12800 (4%)]\tLoss: 3.83589\tAccuracy: 0.26247\n",
            "Epoch: 1 [600/12800 (5%)]\tLoss: 4.54037\tAccuracy: 0.31652\n",
            "Epoch: 1 [700/12800 (5%)]\tLoss: 5.23666\tAccuracy: 0.32281\n",
            "Epoch: 1 [800/12800 (6%)]\tLoss: 5.95245\tAccuracy: 0.30366\n",
            "Epoch: 1 [900/12800 (7%)]\tLoss: 6.69539\tAccuracy: 0.27105\n",
            "Epoch: 1 [1000/12800 (8%)]\tLoss: 7.42679\tAccuracy: 0.26889\n",
            "Epoch: 1 [1100/12800 (9%)]\tLoss: 8.12384\tAccuracy: 0.31241\n",
            "Epoch: 1 [1200/12800 (9%)]\tLoss: 8.82235\tAccuracy: 0.29893\n",
            "Epoch: 1 [1300/12800 (10%)]\tLoss: 9.49604\tAccuracy: 0.33111\n",
            "Epoch: 1 [1400/12800 (11%)]\tLoss: 10.24869\tAccuracy: 0.24748\n",
            "Epoch: 1 [1500/12800 (12%)]\tLoss: 10.98565\tAccuracy: 0.26493\n",
            "Epoch: 1 [1600/12800 (12%)]\tLoss: 11.68647\tAccuracy: 0.29414\n",
            "Epoch: 1 [1700/12800 (13%)]\tLoss: 12.33737\tAccuracy: 0.34167\n",
            "Epoch: 1 [1800/12800 (14%)]\tLoss: 13.03701\tAccuracy: 0.30150\n",
            "Epoch: 1 [1900/12800 (15%)]\tLoss: 13.73368\tAccuracy: 0.30043\n",
            "Epoch: 1 [2000/12800 (16%)]\tLoss: 14.41750\tAccuracy: 0.31806\n",
            "Epoch: 1 [2100/12800 (16%)]\tLoss: 15.08201\tAccuracy: 0.33039\n",
            "Epoch: 1 [2200/12800 (17%)]\tLoss: 15.77059\tAccuracy: 0.30752\n",
            "Epoch: 1 [2300/12800 (18%)]\tLoss: 16.48577\tAccuracy: 0.28039\n",
            "Epoch: 1 [2400/12800 (19%)]\tLoss: 17.19336\tAccuracy: 0.29219\n",
            "Epoch: 1 [2500/12800 (20%)]\tLoss: 17.91285\tAccuracy: 0.28045\n",
            "Epoch: 1 [2600/12800 (20%)]\tLoss: 18.62691\tAccuracy: 0.28019\n",
            "Epoch: 1 [2700/12800 (21%)]\tLoss: 19.26636\tAccuracy: 0.36666\n",
            "Epoch: 1 [2800/12800 (22%)]\tLoss: 19.98880\tAccuracy: 0.28306\n",
            "Epoch: 1 [2900/12800 (23%)]\tLoss: 20.76216\tAccuracy: 0.22318\n",
            "Epoch: 1 [3000/12800 (23%)]\tLoss: 21.49954\tAccuracy: 0.27269\n",
            "Epoch: 1 [3100/12800 (24%)]\tLoss: 22.17673\tAccuracy: 0.32650\n",
            "Epoch: 1 [3200/12800 (25%)]\tLoss: 22.81529\tAccuracy: 0.36333\n",
            "Epoch: 1 [3300/12800 (26%)]\tLoss: 23.53559\tAccuracy: 0.29187\n",
            "Epoch: 1 [3400/12800 (27%)]\tLoss: 24.22307\tAccuracy: 0.31735\n",
            "Epoch: 1 [3500/12800 (27%)]\tLoss: 24.90935\tAccuracy: 0.31945\n",
            "Epoch: 1 [3600/12800 (28%)]\tLoss: 25.53387\tAccuracy: 0.39251\n",
            "Epoch: 1 [3700/12800 (29%)]\tLoss: 26.21082\tAccuracy: 0.33514\n",
            "Epoch: 1 [3800/12800 (30%)]\tLoss: 26.92216\tAccuracy: 0.29241\n",
            "Epoch: 1 [3900/12800 (30%)]\tLoss: 27.61267\tAccuracy: 0.32152\n",
            "Epoch: 1 [4000/12800 (31%)]\tLoss: 28.25246\tAccuracy: 0.37027\n",
            "Epoch: 1 [4100/12800 (32%)]\tLoss: 28.97868\tAccuracy: 0.28418\n",
            "Epoch: 1 [4200/12800 (33%)]\tLoss: 29.62677\tAccuracy: 0.36013\n",
            "Epoch: 1 [4300/12800 (34%)]\tLoss: 30.33470\tAccuracy: 0.28293\n",
            "Epoch: 1 [4400/12800 (34%)]\tLoss: 31.03820\tAccuracy: 0.29680\n",
            "Epoch: 1 [4500/12800 (35%)]\tLoss: 31.71177\tAccuracy: 0.33491\n",
            "Epoch: 1 [4600/12800 (36%)]\tLoss: 32.37687\tAccuracy: 0.34021\n",
            "Epoch: 1 [4700/12800 (37%)]\tLoss: 33.08682\tAccuracy: 0.29235\n",
            "Epoch: 1 [4800/12800 (38%)]\tLoss: 33.69588\tAccuracy: 0.39650\n",
            "Epoch: 1 [4900/12800 (38%)]\tLoss: 34.36852\tAccuracy: 0.33300\n",
            "Epoch: 1 [5000/12800 (39%)]\tLoss: 34.98855\tAccuracy: 0.38835\n",
            "Epoch: 1 [5100/12800 (40%)]\tLoss: 35.65205\tAccuracy: 0.33876\n",
            "Epoch: 1 [5200/12800 (41%)]\tLoss: 36.30848\tAccuracy: 0.34828\n",
            "Epoch: 1 [5300/12800 (41%)]\tLoss: 36.95129\tAccuracy: 0.36118\n",
            "Epoch: 1 [5400/12800 (42%)]\tLoss: 37.65411\tAccuracy: 0.30131\n",
            "Epoch: 1 [5500/12800 (43%)]\tLoss: 38.26609\tAccuracy: 0.39174\n",
            "Epoch: 1 [5600/12800 (44%)]\tLoss: 38.90148\tAccuracy: 0.36737\n",
            "Epoch: 1 [5700/12800 (45%)]\tLoss: 39.61001\tAccuracy: 0.29414\n",
            "Epoch: 1 [5800/12800 (45%)]\tLoss: 40.29640\tAccuracy: 0.31673\n",
            "Epoch: 1 [5900/12800 (46%)]\tLoss: 40.90017\tAccuracy: 0.39814\n",
            "Epoch: 1 [6000/12800 (47%)]\tLoss: 41.60485\tAccuracy: 0.29639\n",
            "Epoch: 1 [6100/12800 (48%)]\tLoss: 42.31425\tAccuracy: 0.29178\n",
            "Epoch: 1 [6200/12800 (48%)]\tLoss: 42.97671\tAccuracy: 0.33808\n",
            "Epoch: 1 [6300/12800 (49%)]\tLoss: 43.62775\tAccuracy: 0.35039\n",
            "Epoch: 1 [6400/12800 (50%)]\tLoss: 44.37913\tAccuracy: 0.24925\n",
            "Epoch: 1 [6500/12800 (51%)]\tLoss: 45.08297\tAccuracy: 0.29662\n",
            "Epoch: 1 [6600/12800 (52%)]\tLoss: 45.75340\tAccuracy: 0.33099\n",
            "Epoch: 1 [6700/12800 (52%)]\tLoss: 46.41569\tAccuracy: 0.33628\n",
            "Epoch: 1 [6800/12800 (53%)]\tLoss: 47.10247\tAccuracy: 0.31336\n",
            "Epoch: 1 [6900/12800 (54%)]\tLoss: 47.71339\tAccuracy: 0.38948\n",
            "Epoch: 1 [7000/12800 (55%)]\tLoss: 48.36478\tAccuracy: 0.34811\n",
            "Epoch: 1 [7100/12800 (55%)]\tLoss: 49.01663\tAccuracy: 0.34848\n",
            "Epoch: 1 [7200/12800 (56%)]\tLoss: 49.74743\tAccuracy: 0.26973\n",
            "Epoch: 1 [7300/12800 (57%)]\tLoss: 50.39947\tAccuracy: 0.34849\n",
            "Epoch: 1 [7400/12800 (58%)]\tLoss: 51.14671\tAccuracy: 0.25300\n",
            "Epoch: 1 [7500/12800 (59%)]\tLoss: 51.71939\tAccuracy: 0.42796\n",
            "Epoch: 1 [7600/12800 (59%)]\tLoss: 52.39128\tAccuracy: 0.32861\n",
            "Epoch: 1 [7700/12800 (60%)]\tLoss: 53.06932\tAccuracy: 0.32261\n",
            "Epoch: 1 [7800/12800 (61%)]\tLoss: 53.70307\tAccuracy: 0.36673\n",
            "Epoch: 1 [7900/12800 (62%)]\tLoss: 54.33009\tAccuracy: 0.37262\n",
            "Epoch: 1 [8000/12800 (62%)]\tLoss: 55.04596\tAccuracy: 0.28426\n",
            "Epoch: 1 [8100/12800 (63%)]\tLoss: 55.76874\tAccuracy: 0.27670\n",
            "Epoch: 1 [8200/12800 (64%)]\tLoss: 56.41058\tAccuracy: 0.35805\n",
            "Epoch: 1 [8300/12800 (65%)]\tLoss: 57.08253\tAccuracy: 0.32837\n",
            "Epoch: 1 [8400/12800 (66%)]\tLoss: 57.77149\tAccuracy: 0.31105\n",
            "Epoch: 1 [8500/12800 (66%)]\tLoss: 58.37268\tAccuracy: 0.39866\n",
            "Epoch: 1 [8600/12800 (67%)]\tLoss: 59.00113\tAccuracy: 0.37201\n",
            "Epoch: 1 [8700/12800 (68%)]\tLoss: 59.67830\tAccuracy: 0.32301\n",
            "Epoch: 1 [8800/12800 (69%)]\tLoss: 60.26638\tAccuracy: 0.41224\n",
            "Epoch: 1 [8900/12800 (70%)]\tLoss: 60.90942\tAccuracy: 0.35733\n",
            "Epoch: 1 [9000/12800 (70%)]\tLoss: 61.53749\tAccuracy: 0.37228\n",
            "Epoch: 1 [9100/12800 (71%)]\tLoss: 62.17419\tAccuracy: 0.36367\n",
            "Epoch: 1 [9200/12800 (72%)]\tLoss: 62.73209\tAccuracy: 0.44275\n",
            "Epoch: 1 [9300/12800 (73%)]\tLoss: 63.39865\tAccuracy: 0.33385\n",
            "Epoch: 1 [9400/12800 (73%)]\tLoss: 64.08691\tAccuracy: 0.31189\n",
            "Epoch: 1 [9500/12800 (74%)]\tLoss: 64.76366\tAccuracy: 0.32361\n",
            "Epoch: 1 [9600/12800 (75%)]\tLoss: 65.41194\tAccuracy: 0.35193\n",
            "Epoch: 1 [9700/12800 (76%)]\tLoss: 66.02919\tAccuracy: 0.38308\n",
            "Epoch: 1 [9800/12800 (77%)]\tLoss: 66.66196\tAccuracy: 0.36760\n",
            "Epoch: 1 [9900/12800 (77%)]\tLoss: 67.25974\tAccuracy: 0.40244\n",
            "Epoch: 1 [10000/12800 (78%)]\tLoss: 67.87712\tAccuracy: 0.38293\n",
            "Epoch: 1 [10100/12800 (79%)]\tLoss: 68.51559\tAccuracy: 0.36176\n",
            "Epoch: 1 [10200/12800 (80%)]\tLoss: 69.15437\tAccuracy: 0.36146\n",
            "Epoch: 1 [10300/12800 (80%)]\tLoss: 69.79592\tAccuracy: 0.35866\n",
            "Epoch: 1 [10400/12800 (81%)]\tLoss: 70.42597\tAccuracy: 0.37035\n",
            "Epoch: 1 [10500/12800 (82%)]\tLoss: 71.04325\tAccuracy: 0.38297\n",
            "Epoch: 1 [10600/12800 (83%)]\tLoss: 71.67433\tAccuracy: 0.36911\n",
            "Epoch: 1 [10700/12800 (84%)]\tLoss: 72.37355\tAccuracy: 0.30089\n",
            "Epoch: 1 [10800/12800 (84%)]\tLoss: 73.03008\tAccuracy: 0.34372\n",
            "Epoch: 1 [10900/12800 (85%)]\tLoss: 73.68468\tAccuracy: 0.34564\n",
            "Epoch: 1 [11000/12800 (86%)]\tLoss: 74.32028\tAccuracy: 0.36465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cROYfWFQivC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.cpu()\n",
        "fig, axeslist = plt.subplots(ncols=5,nrows=1)\n",
        "\n",
        "x,y =train_dataset[100]\n",
        "axeslist.ravel()[4].imshow(x.detach().numpy().reshape(101,101,3))\n",
        "axeslist.ravel()[0].imshow(x.detach().numpy()[0],cmap='gray')\n",
        "print(x.shape)\n",
        "z = model(x.view(1,3,101,101))\n",
        "axeslist.ravel()[1].imshow(z.squeeze(0).detach().numpy(),cmap='gray')\n",
        "print(z.shape)\n",
        "def getImg(x):\n",
        "    x = x.view(1,3,101,101)\n",
        "    x = model(x).detach().squeeze(0).numpy()[0]\n",
        "    x = (x-x.mean()/(x.max()-x.min())) +1\n",
        "    a = np.expand_dims(x, axis = 2)\n",
        "    a = np.concatenate((a, a, a), axis = 2)\n",
        "    return a\n",
        "print(y.shape)\n",
        "predict = (F.sigmoid(z) > 0.5).detach().numpy().squeeze(0)\n",
        "axeslist.ravel()[3].imshow(predict,cmap='gray')\n",
        "\n",
        "\n",
        "# axeslist.ravel()[1].imshow(getImg(y),cmap='gray')\n",
        "\n",
        "axeslist.ravel()[2].imshow(y.detach().numpy(),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vro1Wre1jVSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "|x = torch.randn(1,3,101,101)\n",
        "plt.imshow(x.numpy().squeeze(0)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0EoJqaaExKjE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model,'TGSUNetModel61.pt')\n",
        "uploaded = drive.CreateFile({'title': 'TGSUNetModel61.pt'})\n",
        "uploaded.SetContentFile('TGSUNetModel61.pt')\n",
        "uploaded.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EPGsTKVsxXwm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "downloaded = drive.CreateFile({'id': '1Gs1AlcVL9WNlgHQPSpNvbFtSbqNFZIdu'})\n",
        "downloaded.GetContentFile('test.zip')\n",
        "with ZipFile(\"test.zip\", 'r') as z:\n",
        "  z.extractall()\n",
        "os.remove(\"test.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1YRChhpixaZJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TGSSaltTestDataset(Dataset):\n",
        "    def __init__(self, image_dir,test_csv):\n",
        "        self.image_dir=image_dir\n",
        "        self.filter = np.array([(0,-1,-1,-1),(1,0,0,0),(1,0,1,0),(0,1,0,1)])/8\n",
        "        self.input = pd.read_csv(test_csv)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir,self.input.iloc[idx,0]+\".png\")\n",
        "        img = cv2.imread(img_name)\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "        img1 = cv2.filter2D(img,-1,self.filter)\n",
        "        \n",
        "        sigma=0.33\n",
        "        \n",
        "        \n",
        "        v = np.median(img1)\n",
        "\n",
        "        #---- apply automatic Canny edge detection using the computed median----\n",
        "        lower = int(max(0, (1.0 - sigma) * v))\n",
        "        upper = int(min(255, (1.0 + sigma) * v))\n",
        "        img1 = cv2.Canny(img1, lower, upper)\n",
        "        \n",
        "        img2 = (cv2.Laplacian(img, cv2.CV_32F) + 127.0)\n",
        "#         img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
        "        \n",
        "        img = torch.tensor(img).view(1,101,101).float()\n",
        "        img1 = torch.tensor(img1).view(1,101,101).float()\n",
        "        img2 = torch.tensor(img2).view(1,101,101)\n",
        "        \n",
        "        img = torch.cat((img,img1,img2),dim=0)/255\n",
        "        \n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1CoZ2ZFZyFFi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "test_dataset = TGSSaltTestDataset(\"test/images\",\"test.csv\")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Cyx-txnwXum",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(test_loader):\n",
        "    test_pred=torch.IntTensor().cuda()\n",
        "    model.eval()\n",
        "    for batch_idx, data in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "#             target = target.cuda()\n",
        "        # forward\n",
        "        output = model(data)\n",
        "        predictx = (F.sigmoid(output) > 0.5).int()\n",
        "        test_pred=torch.cat((test_pred,predictx.view(batch_size,101,101)),dim=0)\n",
        "    return test_pred.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3bNyLN02yLn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict=prediciton(test_loader)\n",
        "predict=predict.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRC4rYuYySkY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels = img.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Nq9-6hHyV-t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset.input.drop(\"z\",axis=1,inplace=True)\n",
        "predicted = predict(test_loader)\n",
        "test_dataset.input['rle_mask']=np.nan\n",
        "for i in range(len(test_dataset)):\n",
        "    test_dataset.input[\"rle_mask\"][i]=rle_encode(predicted[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ScpRh0ukyYyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset.input.to_csv(\"submission11.csv\",index=False)\n",
        "uploaded = drive.CreateFile({'title': 'submission11.csv'})\n",
        "uploaded.SetContentFile('submission11.csv')\n",
        "uploaded.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GvAlb7sjspQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py"
      ]
    },
    {
      "metadata": {
        "id": "9VTtLhXttOnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6813cda-929f-49a6-89f0-b77a03748f12"
      },
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "7EbUomFA6KoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46760966-9106-48fe-cfbd-8f1a74b89022"
      },
      "cell_type": "code",
      "source": [
        "torch.tensor(train_dataset[1][2][0])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7608)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "3QMWEl_ijAMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}